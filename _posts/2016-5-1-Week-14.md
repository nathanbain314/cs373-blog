---
layout: post
title: Week 14
---

###April 25 through 29###
###Past week ###
We still had a little bit of work to do on this last portion of the project in order to prepare for the presentation. We still needed to work on the application of the other groups API and we needed create the entire slideshow. Also, Carina and Docker were experiencing problems and we needed to try to work through those in order to keep the website and API up.

###Challenges###
Forcing myself to study is always difficult for me to do.

###Next week###
Next week my team and I will present our website, and then I will study for the test some more. 

###Tip of the week###
There are 186 UTCS public computers. Linux defaults to some computer, formally sloth, but currently skipper, so never log into skipper or linux if you care about performance. Some are connected to the Condor computing cluster, others have multiple people logged in competing for memory and processing power. Last year when I took 439 one of the pintos tests would fail if it took too long, and it would take longer if I was testing on a machine with other users running heavy programs. Last semester in Programming for Performance I needed more reliable results when timing a program, and being on a machine with multiple people competing for resources would create much different results. The UTCS department has a [website](http://apps.cs.utexas.edu/unixlabstatus/) that shows the results of the “uptime” command on each machine. Using this you can find a machine to log into that has a lower load with fewer people, and last year when I was learning unix commands I came up with a janky solution to automate just that.     
      
```
curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep '<td style="background-color: white; text-align:  left;">\|<td style="background-color: yellow; text-align:  left;">' -n | sed 's/[:].*$//' | awk '{ $1+=4} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g') | sed 's/[^>]*\(>.*\)/\1/' | sed 's/[<].*$//' | sed 's/^.//' | sort -n | sed -e '1!d')| sed 's/[:].*$//' | awk '{ $1-=1} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g') | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep '<td style="background-color: white; text-align:  left;">\|<td style="background-color: yellow; text-align:  left;">' -n | sed 's/[:].*$//' | awk '{ $1+=4} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g') | sed 's/[^>]*\(>.*\)/\1/' | sed 's/[<].*$//' | sed 's/^.//' | sort -n | sed -e '1!d')| sed 's/[:].*$//' | awk '{ $1-=1} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g') | sed 's/[^>]*\(>.*\)/\1/' | sed 's/[<].*$//' | sed 's/^.//'| sort -n | sed -e '1d' | sed -e '1!d' | sed "s/.*/\>&\</") | sed -e '1!d' | sed 's/[:].*$//' | awk '{ $1-=3} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g')| sed 's/[^>]*\(>.*\)/\1/' | sed 's/[<].*$//' | sed 's/^.//'
```      
      
Basically it parses the site a couple of times and sorts the data, but I don’t really remember what any part actually does. It works almost all of the time but it doesn’t seem like a good solution. Calling ssh $(command) will ssh you into it from a UTCS computer. Calling ssh username@$(command).cs.utexas.edu will connect to it from any computer.     

